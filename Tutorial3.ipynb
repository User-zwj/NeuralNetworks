{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customization & Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize the activation function\n",
    "\n",
    "Modification based on [Ref](https://medium.com/@chinesh4/custom-activation-function-in-tensorflow-for-deep-neural-networks-from-scratch-tutorial-b12e00652e24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customize activation    \n",
    "\n",
    "#work/python/keras-1\n",
    "\n",
    "#work/DNN/Comparison-activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Gaussin Radial Basis Function](https://en.wikipedia.org/wiki/Radial_basis_function) is defined as\n",
    "$$\n",
    "\\phi(x) = e^{-(\\epsilon r)^2}\n",
    "$$\n",
    "\n",
    "We found out that radial basis function works pretty good as an activation in a network, this will be demonstrated in the following tutorials. The raial basis function only makes a difference in an interval close to the center which is similar to the idea of \"on-and-off\" of an activation function.\n",
    "\n",
    "In a dimension 1 space, let us set activation to be $\\phi(x)=e^{-x^2}$. Since this is not in TensorFlow's list of activation functions https://www.tensorflow.org/api_docs/python/tf/keras/activations, we will use it for demonstration of customizing activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "#define the activation function\n",
    "def rbf(x):\n",
    "    return tf.math.exp(-x**2)\n",
    "\n",
    "#######################################\n",
    "#define the derivative of the activation function\n",
    "def d_rbf(x):\n",
    "    return tf.gradients(rbf,x)\n",
    "\n",
    "#######################################\n",
    "#we couldn't use “d_rbf” as an activation function if we wanted to \n",
    "#because tensorflow doesn't know how to calculate the gradients of that function.\n",
    "def rbf_grad(op, grad):\n",
    "    x = op.inputs[0]\n",
    "    n_gr = d_rbf(x)    #defining the gradient.\n",
    "    return grad * n_gr\n",
    "\n",
    "def py_func(func, inp, Tout, stateful=True, name=None, grad=None):\n",
    "    # Need to generate a unique name to avoid duplicates:\n",
    "    rnd_name = 'PyFuncGrad' + str(np.random.randint(0, 1E+2))\n",
    "    tf.RegisterGradient(rnd_name)(grad)\n",
    "    g = tf.get_default_graph()\n",
    "    with g.gradient_override_map({\"PyFunc\": rnd_name, \"PyFuncStateless\": rnd_name}):\n",
    "        return tf.py_func(func, inp, Tout, stateful=stateful, name=name)\n",
    "    \n",
    "def tf_rbf(x,name=None):\n",
    "    with tf.name_scope(name, \"rbf\", [x]) as name:\n",
    "        y = py_func(rbf,   #forward pass function\n",
    "                    [x],\n",
    "                    [tf.float32],\n",
    "                    name=name,\n",
    "                    grad= rbf_grad) #the function that overrides gradient\n",
    "        y[0].set_shape(x.get_shape())     #when using with the code, it is used to specify the rank of the input.\n",
    "    return y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training data\n",
    "np.random.seed(1)\n",
    "x_train = np.linspace(-40,40,20)\n",
    "y_train = 0.3*x_train**2 + np.random.normal(0, 1, len(x_train)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-2.6122294e-03,  1.6906145e-03, -2.2995217e+00]], dtype=float32),\n",
       " array([-4.2092835e-04,  2.8977118e-04,  4.8410983e+00], dtype=float32),\n",
       " array([[  57.36145 ],\n",
       "        [  59.288876],\n",
       "        [-115.397705]], dtype=float32),\n",
       " array([55.409958], dtype=float32)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rbf = tf.keras.Sequential()\n",
    "model_rbf.add(tf.keras.layers.Dense(3,activation=rbf))     #no need to put quote around rbf\n",
    "model_rbf.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "model_rbf.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(0.1))\n",
    "model_rbf.fit(x_train,y_train, epochs=1000, verbose=0)\n",
    "\n",
    "# model.summary()\n",
    "model_rbf.get_weights()   #test if model is trained successfully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cutomize the loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(3,activation=rbf))     #no need to put quote around rbf\n",
    "model.add(tf.keras.layers.Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customize loss\n",
    "def custom_loss(ytrue,ypred):\n",
    "    val = tf.math.reduce_mean(tf.math.square((ytrue-ypred)/10))\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 4.8343139e-05, -1.8820183e-01,  5.0299735e-05]], dtype=float32),\n",
       " array([1.9036455e-05, 5.2888556e-03, 2.9459905e-05], dtype=float32),\n",
       " array([[ 5.030023 ],\n",
       "        [-4.4802723],\n",
       "        [ 4.8218493]], dtype=float32),\n",
       " array([3.9862227], dtype=float32)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=custom_loss,optimizer=tf.keras.optimizers.Adam(0.01))\n",
    "model.fit(x_train,y_train,epochs=100,verbose=0)\n",
    "\n",
    "model.get_weights()    #test if model is trained successfully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Models](#Models)\n",
    "- [Layers](#Layers)\n",
    "- [Activations](#Activations)\n",
    "- [Fully Connected Networks](#Fully-Connected-Networks)\n",
    "\n",
    "[Ref](https://www.dbooks.org/tensorflow-2-tutorial-1589/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components\n",
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from functools import reduce\n",
    "from matplotlib import pyplot as plt\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "## Generate Needed Data\n",
    "## Ax = b Problem\n",
    "tf.random.set_seed(42)\n",
    "true_weights = tf.constant(list(range(5)), dtype=tf.float32)[:, tf.newaxis]    #x\n",
    "x = tf.constant(tf.random.uniform((32, 5)), dtype=tf.float32)           #A\n",
    "y = tf.constant(x @ true_weights, dtype=tf.float32)            #b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tf.Variable(tf.random.uniform((5, 1)), dtype=tf.float32)\n",
    "y_hat = tf.linalg.matmul(x, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.all(tf.matmul(x, weights) == tf.linalg.matmul(x, weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trial 1: Define this linear regression model using class.\n",
    "\n",
    "With the class, we can evaluate `Ax` and access `x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(object):\n",
    "    def __init__(self, num_parameters):\n",
    "        self._weights = tf.Variable(tf.random.uniform((num_parameters,1)), dtype=tf.float32)\n",
    "    \n",
    "    @tf.function\n",
    "    def __call__(self, x):\n",
    "        return tf.linalg.matmul(x, self._weights)\n",
    "    \n",
    "    @property\n",
    "    def variables(self):\n",
    "        return self._weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LinearRegression(5)\n",
    "# model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we decorated the `__call__` method with `tf.function`, hence a graph will\n",
    "be generated to back up the computation.\n",
    "\n",
    "With this class, we can rewrite the training code as:\n",
    "\n",
    "#### Trial 1: Train the model\n",
    "\n",
    "Gradient descent method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared loss at iteration    0 is 18.7201\n",
      "mean squared loss at iteration  200 is 0.0325\n",
      "mean squared loss at iteration  400 is 0.0017\n",
      "mean squared loss at iteration  600 is 0.0002\n",
      "mean squared loss at iteration  800 is 0.0000\n",
      "mean squared loss at iteration 1000 is 0.0000\n",
      "<tf.Variable 'Variable:0' shape=(5, 1) dtype=float32, numpy=\n",
      "array([[-3.1365452e-03],\n",
      "       [ 1.0065703e+00],\n",
      "       [ 2.0000944e+00],\n",
      "       [ 3.0032609e+00],\n",
      "       [ 3.9934685e+00]], dtype=float32)>\n",
      "time took: 0.35858607292175293 seconds\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression(5)\n",
    "\n",
    "@tf.function\n",
    "def train_step():\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_hat = model(x)\n",
    "        loss = tf.reduce_mean(tf.square(y - y_hat))\n",
    "    gradients = tape.gradient(loss, model.variables)\n",
    "    model.variables.assign_add(tf.constant([-0.05], dtype=tf.float32)* gradients)\n",
    "    return loss\n",
    "\n",
    "t0 = time.time()\n",
    "for iteration in range(1001):\n",
    "    loss = train_step()\n",
    "    if not (iteration % 200):\n",
    "        print('mean squared loss at iteration {:4d} is {:5.4f}'.format(iteration, loss))\n",
    "\n",
    "pprint(model.variables)\n",
    "print('time took: {} seconds'.format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we still decorated the `train_step` function with `tf.function`. The reason is\n",
    "that <font color=red>the loss and gradient calculation can also benefit from graphs</font>.\n",
    "This simple model class works fine. But it can be better if we subclass from\n",
    "`tf.keras.Model`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trial 2: Define this linear regression model using class as subclass from tf.keras.Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(tf.keras.Model):\n",
    "    def __init__(self, num_parameters, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._weights = tf.Variable(tf.random.uniform((num_parameters,1)), dtype=tf.float32)\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        return tf.linalg.matmul(x, self._weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model class has a few differences compared with the organic version. First is that we\n",
    "implemented the call method rather than the dunder version of it. Under the hood,\n",
    "`tf.keras.Model`'s `__call__` method is a wrapper over this `call` method, and it is\n",
    "performing, among many other things, things like converting inputs to tensors and graph\n",
    "building. The second thing is that we dropped the accessor for the variables because we\n",
    "inherited one.\n",
    "\n",
    "\n",
    "With this subclass model, we need to modify the training code a bit to make it work. The\n",
    "`.variables` accessor from `tf.keras.Model` gives us a collection of references to\n",
    "the model's variables, to accommodate complex models with many sets of variables. So,\n",
    "as a result, the corresponding gradients will be a collection too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.7402308 ],\n",
       "        [0.33938193],\n",
       "        [0.5692506 ],\n",
       "        [0.44811392],\n",
       "        [0.29285502]], dtype=float32)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression(5)\n",
    "# model(x)\n",
    "model.get_weights()   #only the value in model.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(5, 1) dtype=float32, numpy=\n",
       " array([[0.7402308 ],\n",
       "        [0.33938193],\n",
       "        [0.5692506 ],\n",
       "        [0.44811392],\n",
       "        [0.29285502]], dtype=float32)>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.variables   #this is a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trial 2: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared loss at iteration    0 is 14.3411\n",
      "mean squared loss at iteration  200 is 0.0294\n",
      "mean squared loss at iteration  400 is 0.0016\n",
      "mean squared loss at iteration  600 is 0.0002\n",
      "mean squared loss at iteration  800 is 0.0000\n",
      "mean squared loss at iteration 1000 is 0.0000\n",
      "[<tf.Variable 'Variable:0' shape=(5, 1) dtype=float32, numpy=\n",
      "array([[-3.2304246e-03],\n",
      "       [ 1.0066777e+00],\n",
      "       [ 2.0000989e+00],\n",
      "       [ 3.0033290e+00],\n",
      "       [ 3.9933815e+00]], dtype=float32)>]\n",
      "time took: 0.27019238471984863 seconds\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression(5)\n",
    "\n",
    "@tf.function\n",
    "def train_step():\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_hat = model(x)\n",
    "        loss = tf.reduce_mean(tf.square(y - y_hat))\n",
    "    gradients = tape.gradient(loss, model.variables)\n",
    "    for g, v in zip(gradients, model.variables):      #if use the above way to update variables, error; see below\n",
    "        v.assign_add(tf.constant([-0.05], dtype=tf.float32) * g)\n",
    "    return loss\n",
    "\n",
    "t0 = time.time()\n",
    "for iteration in range(1001):\n",
    "    loss = train_step()\n",
    "    if not (iteration % 200):\n",
    "        print('mean squared loss at iteration {:4d} is {:5.4f}'.format(iteration, loss))\n",
    "        \n",
    "pprint(model.variables)\n",
    "print('time took: {} seconds'.format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment the following to see the error message\n",
    "\n",
    "# model = LinearRegression(5)\n",
    "\n",
    "# @tf.function\n",
    "# def train_step():\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         y_hat = model(x)\n",
    "#         loss = tf.reduce_mean(tf.square(y - y_hat))\n",
    "#     gradients = tape.gradient(loss, model.variables)    #model.variables is a list\n",
    "#     model.variables.assign_add(tf.constant([-0.05], dtype=tf.float32) * gradients)\n",
    "#     return loss\n",
    "\n",
    "# t0 = time.time()\n",
    "# for iteration in range(1001):\n",
    "#     loss = train_step()\n",
    "#     if not (iteration % 200):\n",
    "#         print('mean squared loss at iteration {:4d} is {:5.4f}'.format(iteration, loss))\n",
    "        \n",
    "# pprint(model.variables)\n",
    "# print('time took: {} seconds'.format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through subclassing `tf.keras.Model` , we get to use many methods from it, like print\n",
    "out a summary and the Keras model training/testing methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"linear_regression_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Total params: 5\n",
      "Trainable params: 5\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "[4.5830074668629095e-06, 0.0017235130071640015]\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())\n",
    "model.compile(loss='mse', metrics=['mae'])\n",
    "print(model.evaluate(x, y, verbose=-1))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=4.5830075e-06>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## MSE\n",
    "# tf.reduce_mean(tf.keras.losses.MSE(model(x),y))\n",
    "tf.reduce_mean(tf.square(model(x) - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.001723513>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## MAE\n",
    "tf.reduce_mean(tf.math.abs(model(x) - y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also try using the `.fit` API to train our linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18.954912185668945,\n",
      " 0.02976365201175213,\n",
      " 0.0010764360195025802,\n",
      " 3.935765562346205e-05,\n",
      " 1.450204877073702e-06,\n",
      " 5.471034825177412e-08]\n",
      "[<tf.Variable 'Variable:0' shape=(5, 1) dtype=float32, numpy=\n",
      "array([[5.6911039e-04],\n",
      "       [1.0001738e+00],\n",
      "       [1.9999688e+00],\n",
      "       [2.9999278e+00],\n",
      "       [3.9994378e+00]], dtype=float32)>]\n",
      "Time took: 0.8120768070220947 seconds\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression(5)\n",
    "model.compile(optimizer='SGD', loss='mse')\n",
    "model.optimizer.lr.assign(.05)\n",
    "t0 = time.time()\n",
    "history = model.fit(x, y, epochs=1001, verbose=0)\n",
    "pprint(history.history['loss'][::200])\n",
    "pprint(model.variables)\n",
    "print('Time took: {} seconds'.format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly we recovered the ground truth, but it took significantly more time\n",
    "compared to our custom training loop, it is probably due to the convenient `.fit` API is\n",
    "doing a bunch more stuff than updating parameters.\n",
    "\n",
    "#### Trial 3: Define this linear regression model using class as subclass from tf.keras.Model\n",
    "\n",
    "Now, let's spice up the model a bit by adding a additional useless bias term and initialize it\n",
    "with a large value. Ideally, after training, this bias term would become an insignificantly\n",
    "small number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'Variable:0' shape=(5, 1) dtype=float32, numpy=\n",
      "array([[-2.8140063],\n",
      "       [-4.8734136],\n",
      "       [-2.0642996],\n",
      "       [-1.3286525],\n",
      "       [ 2.171379 ]], dtype=float32)>,\n",
      " <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([9.778882], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "### Ax + c = b \n",
    "class LinearRegressionV2(tf.keras.Model):\n",
    "    def __init__(self, num_parameters, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._weights = tf.Variable(tf.random.uniform((num_parameters,1)), dtype=tf.float32)\n",
    "        self._bias = tf.Variable([100], dtype=tf.float32)\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        return tf.linalg.matmul(x, self._weights) + self._bias\n",
    "    \n",
    "model = LinearRegressionV2(5)\n",
    "@tf.function\n",
    "def train_step():\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_hat = model(x)      ##here is the key!!!\n",
    "        loss = tf.reduce_mean(tf.square(y - y_hat))\n",
    "    gradients = tape.gradient(loss, model.variables)\n",
    "    for g, v in zip(gradients, model.variables):      #if use the above way to update variables, error; see below\n",
    "        v.assign_add(tf.constant([-0.05], dtype=tf.float32) * g)\n",
    "    return loss\n",
    "\n",
    "t0 = time.time()\n",
    "for iteration in range(1001):\n",
    "    loss = train_step()\n",
    "\n",
    "pprint(model.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, something is very wrong, the bias term is not updated at all. Guess where might be\n",
    "the problem? It's in the `train_step` function. Since the input signature has not\n",
    "changed(there is none), the function is been lazy and did not realize it should do a\n",
    "retracing. Thus it is training with the old graph, in which the bias term simply does not\n",
    "exist!\n",
    "\n",
    "#### Trial 4: Define this linear regression model using class as subclass from tf.keras.Model\n",
    "\n",
    "To address this issue, we can make `model` as an input to `train_step`, so that when\n",
    "the function is invoked with a different model it will create or grab a graph accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'Variable:0' shape=(5, 1) dtype=float32, numpy=\n",
      "array([[-8.0306112e-04],\n",
      "       [ 1.0025039e+00],\n",
      "       [ 2.0000150e+00],\n",
      "       [ 3.0011511e+00],\n",
      "       [ 3.9972730e+00]], dtype=float32)>]\n",
      "[<tf.Variable 'Variable:0' shape=(5, 1) dtype=float32, numpy=\n",
      "array([[-2.6643013e-03],\n",
      "       [ 9.9406248e-01],\n",
      "       [ 1.9960064e+00],\n",
      "       [ 2.9956589e+00],\n",
      "       [ 3.9983623e+00]], dtype=float32)>,\n",
      " <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([0.0096044], dtype=float32)>]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def train_step(model):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_hat = model(x)\n",
    "        loss = tf.reduce_mean(tf.square(y - y_hat))\n",
    "    gradients = tape.gradient(loss, model.variables)\n",
    "    for g, v in zip(gradients, model.variables):\n",
    "        v.assign_add(tf.constant([-0.05], dtype=tf.float32) * g)\n",
    "    return loss\n",
    "\n",
    "model = LinearRegression(5)     #Ax=b\n",
    "for iteration in range(1001):\n",
    "    loss = train_step(model)\n",
    "pprint(model.variables)\n",
    "\n",
    "model = LinearRegressionV2(5)    #Ax+c=b\n",
    "for iteration in range(5001):\n",
    "    loss = train_step(model)\n",
    "pprint(model.variables)\n",
    "\n",
    "print(train_step._get_tracing_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We typically use the Model class for the overall model architecture, that is how may\n",
    "combine many smaller computational units to do the job. It is ok to code up small units\n",
    "with `tf.keras.Model` and then combine them, but since we won't really need many of\n",
    "the model specific functionalities with these smaller building blocks(for example, its\n",
    "unlikely we would ever want to call the .fit method on a unit). It is better to use the\n",
    "`tf.keras.layers.Layer` class. Actually, the Model class is a wrapper over the Layer\n",
    "class.\n",
    "\n",
    "Lets say that we want to 'upgrade' the linear regression model to be a composition of a\n",
    "few linear transformations. We can code up the linear transformation as a Layer class and\n",
    "then just combine a bunch of its instances of it them in one model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trial 1: Define the linear regression model using class as subclass from tf.keras.layers.Layer\n",
    "\n",
    "Define repeated linear regression (All above is just 1 single linear regression)\n",
    "\n",
    "Note: In trial 1, we need to specify the input, output shape for each linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_inputs, num_outputs, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._weights = tf.Variable(tf.random.uniform((num_inputs, num_outputs)), dtype=tf.float32)\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        return tf.linalg.matmul(x, self._weights)\n",
    "\n",
    "class Regression(tf.keras.Model):\n",
    "    def __init__(self, num_inputs_per_layer, num_outputs_per_layer, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._layers = [Linear(num_inputs, num_outputs) for (num_inputs, num_outputs) in zip(num_inputs_per_layer, num_outputs_per_layer)]\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        for layer in self._layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `Linear` class definition above, we swapped the super class and generalized it with\n",
    "the option to specify output size. In the `Regression` model class, we have the option to\n",
    "use <font color=red> one or a chain of this Linear layers</font>. One obvious benefit with this set up, is that we\n",
    "now separated the concern of how individual computing units should work and the overall\n",
    "architecture design of the model. We use Layer to handle the first, and use Model to\n",
    "tackle the latter.\n",
    "\n",
    "Let's see if the model is trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error is:  1.2591481e-06\n"
     ]
    }
   ],
   "source": [
    "model = Regression([5, 3], [3, 1])\n",
    "for iteration in range(1001):\n",
    "    loss = train_step(model)    #x,y are in train_step\n",
    "print('Mean absolute error is: ', tf.reduce_mean(tf.abs(y - model(x))).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.37866265, -0.01718613, -0.1357575 ],\n",
       "        [ 0.63600516,  0.05279344,  0.31817758],\n",
       "        [ 0.31306508,  0.25236115,  0.93384993],\n",
       "        [ 0.848499  ,  0.3870639 ,  1.2533945 ],\n",
       "        [ 0.55826724,  1.226812  ,  1.5695485 ]], dtype=float32),\n",
       " array([[0.65006894],\n",
       "        [0.76955056],\n",
       "        [1.7157797 ]], dtype=float32)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.37866265, -0.01718613, -0.1357575 ],\n",
       "        [ 0.63600516,  0.05279344,  0.31817758],\n",
       "        [ 0.31306508,  0.25236115,  0.93384993],\n",
       "        [ 0.848499  ,  0.3870639 ,  1.2533945 ],\n",
       "        [ 0.55826724,  1.226812  ,  1.5695485 ]], dtype=float32)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem with this linear layer is that it needs the complete sizing information and\n",
    "allocates resources for all the variables upfront. Ideally, we want it to be a bit lazy, it\n",
    "should calculate variable sizes and occupy resources only when needed. To archive this,\n",
    "we implement the `build` method which will handle the variable initialization. The\n",
    "`build` method can be explicitly called, or it will be invoked automatically the first time\n",
    "there is data flow to it. With this, the constructor now only stores the hyperparameters for\n",
    "the layer.\n",
    "\n",
    "#### Trial 2: Define the linear regression model using class as subclass from tf.keras.layers.Layer\n",
    "\n",
    "Define repeated linear regression \n",
    "\n",
    "Note: In trial 2, we only need to specify the output shape for each linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Mean absolute error is:  2.9111395\n",
      "[<tf.Variable 'linear_2/Variable:0' shape=(5, 3) dtype=float32, numpy=\n",
      "array([[-0.18512696,  0.37053937,  0.03198682],\n",
      "       [ 0.30532497,  0.07832876,  0.09844765],\n",
      "       [ 0.10485165,  0.1986344 ,  0.6662489 ],\n",
      "       [ 0.5690766 , -0.81485623,  0.43635607],\n",
      "       [ 0.33452773, -0.61480993,  1.0721676 ]], dtype=float32)>,\n",
      " <tf.Variable 'linear_3/Variable:0' shape=(3, 2) dtype=float32, numpy=\n",
      "array([[ 0.41160345, -0.7802119 ],\n",
      "       [ 0.29393652,  0.3423354 ],\n",
      "       [-0.71737754, -0.8620656 ]], dtype=float32)>,\n",
      " <tf.Variable 'linear_4/Variable:0' shape=(2, 1) dtype=float32, numpy=\n",
      "array([[-0.5296563],\n",
      "       [-1.1699201]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "### Linear!!! Build!!!\n",
    "class Linear(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(Linear, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self._weights = self.add_weight(shape=(input_shape[-1], self.units))  #for the variable\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        output = tf.linalg.matmul(x, self._weights)\n",
    "        return output\n",
    "\n",
    "class Regression(tf.keras.Model):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._layers = [Linear(unit) for unit in units]    #self._layers stores all the operations\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        for layer in self._layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "model = Regression([3, 2, 1])  #num of output for each layer\n",
    "\n",
    "pprint(model.variables) # should be empty\n",
    "\n",
    "for iteration in range(1001):\n",
    "    loss = train_step(model)\n",
    "\n",
    "print('Mean absolute error is: ', tf.reduce_mean(tf.abs(y - model(x))).numpy())\n",
    "pprint(model.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow has a lot of layer options, we will cover a sample of them in later application\n",
    "specific chapters. For now, we will just quickly see if the linear layer(called Dense ) from\n",
    "Tensorflow works the same.\n",
    "\n",
    "#### Trial 3: Define the linear regression model using Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error is:  4.7280407\n",
      "[<tf.Variable 'dense/kernel:0' shape=(5, 3) dtype=float32, numpy=\n",
      "array([[ 0.04548058, -0.29876846, -0.48789525],\n",
      "       [ 0.14477429,  0.01200902, -0.2724674 ],\n",
      "       [-0.16175295,  0.45236146,  0.31300414],\n",
      "       [ 0.11724836,  0.45436904,  0.62382376],\n",
      "       [ 0.90286434,  0.3896694 ,  0.4262347 ]], dtype=float32)>,\n",
      " <tf.Variable 'dense_1/kernel:0' shape=(3, 2) dtype=float32, numpy=\n",
      "array([[ 0.26604038, -0.5214442 ],\n",
      "       [-0.745106  , -0.4471392 ],\n",
      "       [ 0.56456757, -0.38512182]], dtype=float32)>,\n",
      " <tf.Variable 'dense_2/kernel:0' shape=(2, 1) dtype=float32, numpy=\n",
      "array([[-0.05406497],\n",
      "       [-0.78460944]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "class Regression(tf.keras.Model):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._layers = [tf.keras.layers.Dense(unit, use_bias=False) for unit in units] # the only change\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        for layer in self._layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "model = Regression([3,2,1])\n",
    "for iteration in range(1001):\n",
    "    loss = train_step(model)\n",
    "    \n",
    "print('Mean absolute error is: ', tf.reduce_mean(tf.abs(y - model(x))).numpy())\n",
    "pprint(model.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our newest model is a composition of two linear transformation, but the composition of\n",
    "two linear transformations is just another linear transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.23143438]\n",
      " [-0.01216887]\n",
      " [ 0.19809997]\n",
      " [ 0.39345315]\n",
      " [ 0.624593  ]], shape=(5, 1), dtype=float32)\n",
      "tf.Tensor(True, shape=(), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "reduced_model = reduce(tf.linalg.matmul, model.variables)  #apply tf.linalg.matmul to the elements in model.variables\n",
    "print(reduced_model)\n",
    "print(tf.reduce_all(tf.abs(model(x) - x @ reduced_model) < 1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       "array([[-0.23143438],\n",
       "       [-0.01216887],\n",
       "       [ 0.19809997],\n",
       "       [ 0.39345315],\n",
       "       [ 0.624593  ]], dtype=float32)>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.variables[0] @ model.variables[1] @ model.variables[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without anything interesting in between, this is just adding unnecessary complexity. The\n",
    "simplest thing to do is to add some non-linear in-place transformation to the intermediate\n",
    "results, and this is call activations.\n",
    "\n",
    "Let's **add activations as layers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error is:  1.1175871e-06\n",
      "[<tf.Variable 'linear_5/Variable:0' shape=(5, 3) dtype=float32, numpy=\n",
      "array([[-6.2820163e-07, -3.8094613e-01, -3.3532697e-01],\n",
      "       [ 4.4989979e-01, -6.7443371e-01, -3.8297394e-01],\n",
      "       [ 8.9979744e-01, -8.6434031e-01,  3.5892844e-02],\n",
      "       [ 1.3496974e+00, -3.0281246e-02, -5.0892937e-01],\n",
      "       [ 1.7995927e+00, -2.7777743e-01,  6.9007158e-02]], dtype=float32)>,\n",
      " <tf.Variable 'linear_6/Variable:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[ 2.2227228],\n",
      "       [-0.4953962],\n",
      "       [ 1.098761 ]], dtype=float32)>]\n",
      "tf.Tensor(\n",
      "[[-0.17972632]\n",
      " [ 0.9133176 ]\n",
      " [ 2.467629  ]\n",
      " [ 2.4558127 ]\n",
      " [ 4.213428  ]], shape=(5, 1), dtype=float32)\n",
      "tf.Tensor(False, shape=(), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "class ReLU(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        return tf.maximum(tf.constant(0, x.dtype), x)\n",
    "    \n",
    "class NeuralNetwork(tf.keras.Model):\n",
    "    def __init__(self, units, last_linear=True, **kwargs):     #last_linear: whether or not apply activation to the last output layer\n",
    "        super().__init__(**kwargs)\n",
    "        layers = []\n",
    "        n = len(units)    #number of layers in total\n",
    "        for i, unit in enumerate(units):\n",
    "            layers.append(Linear(unit))\n",
    "            if i < n - 1 or not last_linear:  #last_linear=False: apply activation to output layer\n",
    "                layers.append(ReLU())\n",
    "        self._layers = layers\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        for layer in self._layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "model = NeuralNetwork([3, 1])\n",
    "for iteration in range(1001):\n",
    "    loss = train_step(model)\n",
    "    \n",
    "print('Mean absolute error is: ', tf.reduce_mean(tf.abs(y - model(x))).numpy())\n",
    "pprint(model.variables)\n",
    "reduced_model = reduce(tf.linalg.matmul, model.variables)\n",
    "print(reduced_model)\n",
    "print(tf.reduce_any(tf.abs(model(x) - x @ reduced_model) < 1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our underline ground truth to be a simple linear model, the added non-linear\n",
    "activations is not really helpful, and in fact it made the optimization harder.\n",
    "\n",
    "Since activation functions usually follows immediately after linear transformations, we can\n",
    "fuse them together, so that the model code can be simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error is:  0.04327987\n",
      "[<tf.Variable 'linear_7/Variable:0' shape=(5, 3) dtype=float32, numpy=\n",
      "array([[ 0.19793922, -0.082941  ,  0.3436416 ],\n",
      "       [ 0.31054464,  0.14825837, -0.34439313],\n",
      "       [ 0.85285133,  0.5470691 , -0.13268518],\n",
      "       [ 1.4554257 ,  0.6286258 ,  0.0634411 ],\n",
      "       [ 1.6481082 , -0.47667718, -1.0370482 ]], dtype=float32)>,\n",
      " <tf.Variable 'linear_7/Variable:0' shape=(3,) dtype=float32, numpy=array([0.09033869, 0.69391346, 1.2568238 ], dtype=float32)>,\n",
      " <tf.Variable 'linear_8/Variable:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[ 1.9302187],\n",
      "       [ 0.4248354],\n",
      "       [-0.9970594]], dtype=float32)>,\n",
      " <tf.Variable 'linear_8/Variable:0' shape=(1,) dtype=float32, numpy=array([0.8035747], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "class Linear(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, use_bias=True, activation='linear', **kwargs):\n",
    "        super(Linear, self).__init__(**kwargs)\n",
    "        self.units = units        #the number of output for each layer\n",
    "        self.use_bias = use_bias  #add bias or not\n",
    "        self.activation = activation\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self._weights = self.add_weight(shape=(input_shape[-1], self.units))\n",
    "        if self.use_bias:\n",
    "            self._bias = self.add_weight(shape=(self.units), initializer='ones')\n",
    "        super().build(input_shape)\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        output = tf.linalg.matmul(x, self._weights)\n",
    "        if self.use_bias:\n",
    "            output += self._bias\n",
    "        if self.activation == 'relu':\n",
    "            output = tf.maximum(tf.constant(0, x.dtype), output)\n",
    "        return output\n",
    "\n",
    "class NeuralNetwork(tf.keras.Model):\n",
    "    def __init__(self, units, use_bias=True, last_linear=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        layers = [Linear(unit, use_bias, 'relu') for unit in units[:-1]]\n",
    "        layers.append(Linear(units[-1], use_bias, 'linear' if last_linear else 'relu'))   #operation for the output layer\n",
    "        self._layers = layers\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        for layer in self._layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "model = NeuralNetwork([3, 1]) #with bias, no activation on the output layer\n",
    "for iteration in range(1001):\n",
    "    loss = train_step(model)\n",
    "print('Mean absolute error is: ', tf.reduce_mean(tf.abs(y - model(x))).numpy())\n",
    "pprint(model.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the code above, we just made a fully connected network, or historically called multi\n",
    "layer perceptron(with out any actual perceptron) as well as feed forward neural network.\n",
    "Its essentially a sequence of linear transformation with in-place non-linear activations\n",
    "sandwiched in between. We usually think of the initial layers as feature extractors that is\n",
    "performing some kind on implicit feature engineering and selection, and think of the last\n",
    "layer as a regressor or classifier per task.\n",
    "\n",
    "Note how we are using a list to host the layers and applying them sequentially in the call\n",
    "method. Lets quickly implement a quality of life improvement model class called\n",
    "`Sequential` to do this. It is pretty much a water down version of\n",
    "`tf.keras.Sequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(tf.keras.Model):\n",
    "    def __init__(self, layers, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._layers = layers\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        for layer in self._layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self, num_hidden_units, num_targets, hidden_activation='relu', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        if type(num_hidden_units) is int: num_hidden_units = [num_hidden_units]\n",
    "        self.feature_extractor = Sequential([tf.keras.layers.Dense(unit, activation=hidden_activation) for unit in num_hidden_units])\n",
    "        self.last_linear = tf.keras.layers.Dense(num_targets, activation='linear')\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        outputs = self.last_linear(features)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to apply our MLP model to a real regression problems: the boston housing\n",
    "dataset shipped with Tensorflow. The dataset is splitted into two sets, a training set and a\n",
    "testing set. We will train the model on training set only, but record the loss on both sets to\n",
    "see if the reduction in training set loss is inline with reduction in the unseen testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
      "57344/57026 [==============================] - 0s 0us/step\n",
      "    iteration  training_loss  testing_loss\n",
      "95        960      84.622253     83.712585\n",
      "96        970      84.622261     83.712280\n",
      "97        980      84.622253     83.712013\n",
      "98        990      84.622253     83.711807\n",
      "99       1000      84.622261     83.711624\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(x_tr, y_tr), (x_te, y_te) = tf.keras.datasets.boston_housing.load_data()\n",
    "y_tr, y_te = map(lambda x: np.expand_dims(x, -1), (y_tr, y_te))\n",
    "x_tr, y_tr, x_te, y_te = map(lambda x: tf.cast(x, tf.float32), (x_tr,\n",
    "y_tr, x_te, y_te))\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = tf.reduce_mean(tf.square(y - model(x)))\n",
    "    gradients = tape.gradient(loss, model.variables)\n",
    "    for g, v in zip(gradients, model.variables):\n",
    "        v.assign_add(tf.constant([-0.01], dtype=tf.float32) * g)\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def test_step(model, x, y):\n",
    "    return tf.reduce_mean(tf.square(y - model(x)))\n",
    "\n",
    "def train(model, n_epochs=1000, his_freq=10):\n",
    "    history = []\n",
    "    for iteration in range(1, n_epochs + 1):\n",
    "        tr_loss = train_step(model, x_tr, y_tr)\n",
    "        te_loss = test_step(model, x_te, y_te)\n",
    "        if not iteration % his_freq:\n",
    "            history.append({\n",
    "            'iteration': iteration,\\\n",
    "            'training_loss': tr_loss.numpy(),\\\n",
    "            'testing_loss': te_loss.numpy()\n",
    "            })\n",
    "    return model, pd.DataFrame(history)\n",
    "\n",
    "mlp, mlp_history = train(MLP(4, 1))\n",
    "pprint(mlp_history.tail())\n",
    "ax = mlp_history.plot(x='iteration', kind='line', logy=True)\n",
    "fig = ax.get_figure()\n",
    "# fig.savefig('ch3_plot_1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may seem that our model has nicely converged. Since there is not much a discrepancy\n",
    "between training set and testing set performance. However if we look at the numbers\n",
    "more closely, they are pretty bad. A simple constant prediction have MSE around 83.\n",
    "What could go wrong? We will look at other optimizers in the next chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(83.24384, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.reduce_mean(tf.square(y_te - tf.reduce_mean(y_te))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
